# Hyper-parameters, which is not directly used by our framework
hparams:
  verbose: --print-args=false --verbose=-4
  exp_name: exp/trans20L-lstm2L-specaug_MWERLoss #trans20L-lstm2L_LMadapt_ILME_target_text_epoch3

#checkpoint_dir: /mnt/lustre02/jiangsu/aispeech/home/hs418/transform_e2e/exp/trans20L-lstm2L_LSCTC 
checkpoint_dir: exp/${hparams.exp_name} #_${loss.name}
#init: /mnt/lustre02/jiangsu/aispeech/home/hs418/transform_e2e/exp/trans20L-lstm2L_LSCTC_baseline/checkpoint
#resume: False
#init: exp/trans20L-lstm2L-adapt_LSCTC_mmi/checkpoint
data:
  # data_dir relative to the current work dir
  data_dir: data/train_aitrans_200820 #target_domain_dir
  dataset:
    data_rspecs:
      feat:
        - template: ark:copy-feats ${hparams.verbose} scp:SPLIT_PYRE/feats.scp ark:- |
            apply-cmvn --norm-means=true --norm-vars=false --utt2spk=ark:SPLIT_PYRE/utt2spk scp:SPLIT_PYRE/cmvn.scp ark:- ark:- |
          time_mask:
            T: 70
            p: 0.2
            num_mask: 2
          freq_mask:
            F: 15
            num_mask: 2
      ali:
        - template: ark:copy-int-vector ark:SPLIT_PYRE/ali4.ark ark:- |
        - template: ark:copy-int-vector scp:SPLIT_PYRE/ali3.scp ark:- |
  collector:
    frame_limit: 10000 ### frame_limit larger
    max_length: 2000
    minibatch_size: 100 ### 1000
  no_split: True
  inplace_split: True

optim:
  optimizer: adam
  weight_decay: 0.01
  nesterov: false
  momentum: 0.9
  lr: 5.0e-05

dist:
  global_optimizer: sync
  merge_size: 1

model:
  name: TransNGRAM 
  ninp: 40
  nproj: 512
  nhid: 2048
  nctc: 2966
  natt: 6983 #6979
  nlayer: 20
  nhead: 8
  ndecode: 2 #2
  nhid_dec: 1024
  activation: relu6
  max_norm: 1000
  dec: lstm
  dropout: 0
  pos_emb: False
  mode: mt

#trainer:
#  max_epoch: 5 #1
loss:
  name: MWERLoss #LSCTC
  alpha: 0.1
  nvocab: 6983 #6979

scheduler:
  name: kaldi #naive 
  warmup_round: 0 #100
  warmup_batches_per_round: 100
